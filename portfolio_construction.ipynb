{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from math import sqrt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import hierarchical_portfolio\n",
    "from pyhrp.hrp import dist, linkage, tree, _hrp\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from cvxpy import cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class bounded k-means\n",
    "# NOT WRITTEN BY THE AUTHORS\n",
    "class ClustersUtils:\n",
    "    \n",
    "    @classmethod\n",
    "    def compute_dist_array(cls, X: np.array) -> np.array:\n",
    "        dist_array = np.array([[np.linalg.norm(x1 - x2)\n",
    "                                for x1 in X]\n",
    "                               for x2 in X])\n",
    "        return dist_array\n",
    "    \n",
    "    @classmethod\n",
    "    def scatter_plot(cls, X: np.array, clusters_in_idxs: [[int]], centroid_idxs: [int] = None):\n",
    "        \"\"\"Only plots first two dimensions\"\"\"\n",
    "        x, y = list(zip(*[[X[c_idx][0], X[c_idx][1]]\n",
    "                          for one_cluster_in_idxs in clusters_in_idxs\n",
    "                          for c_idx in one_cluster_in_idxs]))\n",
    "        c = [color_idx\n",
    "             for color_idx, one_cluster_in_idxs in enumerate(clusters_in_idxs)\n",
    "             for _ in one_cluster_in_idxs]\n",
    "        df = pd.DataFrame({'x': x, 'y': y, 'c': c})\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        for color_idx, cluster_in_idxs in enumerate(clusters_in_idxs):\n",
    "            df_temp = df[df['c'].isin([color_idx])]\n",
    "            plt.plot(df_temp['x'].tolist(), df_temp['y'].tolist(), 'o', label=color_idx, markersize=5)\n",
    "\n",
    "        if centroid_idxs is not None:\n",
    "            x_c, y_c = list(zip(*[[X[c_idx][0], X[c_idx][1]]\n",
    "                                  for c_idx in centroid_idxs]))\n",
    "            plt.plot(x_c, y_c, 'o', color='black', markersize=3)\n",
    "\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.show()\n",
    "        \n",
    "    @classmethod\n",
    "    def plot_families_on_map(cls, points_coords_lng_lat: [[float]], family_size: np.array) -> None:\n",
    "        features = []\n",
    "        color = \"#251782\"\n",
    "        for lng_lat, s in zip(points_coords_lng_lat, family_size):\n",
    "            p = Feature(geometry=GeojsonPoint(lng_lat), \n",
    "                        properties={\"marker-symbol\": int(s), \"marker-color\": color})\n",
    "            features += [p]\n",
    "\n",
    "        feature_collection = FeatureCollection(features=features)\n",
    "        geojsonio.display(json.dumps(feature_collection));\n",
    "        \n",
    "    @classmethod\n",
    "    def plot_clustering_on_map(cls, clusters_in_idxs: [[int]], \n",
    "                               points_coords_lng_lat: [[float]], family_size: np.array) -> None:\n",
    "        features = []\n",
    "        for cluster_idxs in clusters_in_idxs:\n",
    "            color = \"#\" + ''.join(random.choices('0123456789abcdef', k=6))\n",
    "            for idx in cluster_idxs:\n",
    "                properties = {\"marker-symbol\": int(family_size[idx]), \"marker-color\": color}\n",
    "                p = Feature(geometry=GeojsonPoint(points_coords_lng_lat[idx]), properties=properties)\n",
    "                features += [p]\n",
    "\n",
    "        feature_collection = FeatureCollection(features=features)\n",
    "        geojsonio.display(json.dumps(feature_collection));\n",
    "        \n",
    "class BoundedKMeansClustering:\n",
    "    def __init__(self, n_clusters: int, max_cluster_size: int, n_iter: int = 10, n_init: int = 10, plot_every_iteration=False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "        self.n_iter = n_iter\n",
    "        self.n_init = n_init\n",
    "        self.plot_every_iteration = plot_every_iteration\n",
    "\n",
    "        self.n_points = None\n",
    "\n",
    "    def fit(self, X: np.array, weights: np.array, dist_array: np.array = None) -> [float, [[int]]]:\n",
    "        self.n_points = X.shape[0]\n",
    "        dist_array = dist_array if dist_array is not None else ClustersUtils.compute_dist_array(X)\n",
    "\n",
    "        costs, clusters = zip(*[self.fit_one_iteration(X, weights, dist_array) for _ in range(self.n_init)])\n",
    "\n",
    "        if all(np.isnan(np.array(costs))):\n",
    "            return np.nan, np.nan\n",
    "        \n",
    "        best_idx = np.nanargmin(costs)\n",
    "        best_cost = costs[best_idx]\n",
    "        best_clusters = clusters[best_idx]\n",
    "\n",
    "        self.n_points = None\n",
    "        return best_cost, best_clusters\n",
    "\n",
    "    def fit_one_iteration(self, X: np.array, weights: np.array, dist_array: np.array) -> [float, [[int]]]:\n",
    "        # inspired by https://core.ac.uk/download/pdf/61217069.pdf\n",
    "\n",
    "        try:\n",
    "            clusters_in_idxs = self._initialize_clusters(weights, dist_array)\n",
    "            best_clusters = clusters_in_idxs\n",
    "            best_cost = self._get_maximal_mean_dist_in_clusters(dist_array, clusters_in_idxs)\n",
    "\n",
    "            for i in range(self.n_iter):\n",
    "                clusters_in_idxs, maximal_mean_dist_in_clusters = self._optimize_clusters(X, weights, dist_array,\n",
    "                                                                                          clusters_in_idxs)\n",
    "                if clusters_in_idxs == best_clusters:\n",
    "                    # print(f\"Reached a local optimum after {i} iterations.\")\n",
    "                    break\n",
    "                \n",
    "                if maximal_mean_dist_in_clusters < best_cost:\n",
    "                    best_cost = maximal_mean_dist_in_clusters\n",
    "                    best_clusters = clusters_in_idxs\n",
    "                    \n",
    "            if self.plot_every_iteration:\n",
    "                ClustersUtils.scatter_plot(X, best_clusters)\n",
    "\n",
    "        except ValueError:\n",
    "            best_cost, best_clusters = np.nan, np.nan\n",
    "\n",
    "        return best_cost, best_clusters\n",
    "\n",
    "    def _initialize_clusters(self, weights: np.array, dist_array: np.array) -> [[int]]:\n",
    "        centroid_idxs = random.sample(range(self.n_points), self.n_clusters)\n",
    "        clusters_in_idxs = self._assign_points_to_clusters(weights, dist_array, centroid_idxs)\n",
    "        return clusters_in_idxs\n",
    "\n",
    "    def _assign_points_to_clusters(self, weights: np.array, dist_array: np.array, centroid_idxs: [int]) -> [[int]]:\n",
    "        clusters_in_idxs = [[c_idx] for c_idx in centroid_idxs]\n",
    "        cluster_weights = np.array([weights[c_idx] for c_idx in centroid_idxs])\n",
    "\n",
    "        sorted_points_idxs_by_weights = [i for i in np.argsort(-weights,  axis=0) if i not in centroid_idxs]\n",
    "        for p_idx in sorted_points_idxs_by_weights:\n",
    "            is_assigned = False\n",
    "            sorted_cluster_idxs_by_dist = np.argsort(dist_array[p_idx][centroid_idxs])\n",
    "            for c_idx in sorted_cluster_idxs_by_dist:\n",
    "                if cluster_weights[c_idx] + weights[p_idx] <= self.max_cluster_size:\n",
    "                    clusters_in_idxs[c_idx].append(p_idx)\n",
    "                    cluster_weights[c_idx] += weights[p_idx]\n",
    "                    is_assigned = True\n",
    "                    break\n",
    "            if not is_assigned:\n",
    "                raise ValueError(\n",
    "                    f\"Point {p_idx} could not be assigned. Try with more than {self.n_clusters} clusters. \"\n",
    "                    f\"Current_clusters in idxs: {clusters_in_idxs}\")\n",
    "\n",
    "        return clusters_in_idxs\n",
    "\n",
    "    def _get_maximal_mean_dist_in_clusters(self, dist_array: np.array, clusters_in_idxs: [[int]]):\n",
    "        mean_dist_in_all_clusters = [self._get_mean_dist_in_cluster(dist_array, one_cluster_in_idxs)\n",
    "                                     for one_cluster_in_idxs in clusters_in_idxs]\n",
    "        maximal_mean_dist_in_clusters = max(mean_dist_in_all_clusters)\n",
    "        return maximal_mean_dist_in_clusters\n",
    "\n",
    "    def _get_mean_dist_in_cluster(self, dist_array: np.array, cluster_in_idxs: [[int]]) -> float:\n",
    "        cluster_sub_dist_array = dist_array[cluster_in_idxs, :][:, cluster_in_idxs]\n",
    "        cluster_sub_dist_array_triu = np.triu(cluster_sub_dist_array)\n",
    "        cluster_sub_dist_array_triu[cluster_sub_dist_array_triu == 0] = np.nan\n",
    "        mean_dist_in_cluster = np.nanmean(cluster_sub_dist_array)\n",
    "        return mean_dist_in_cluster\n",
    "\n",
    "    def _optimize_clusters(self, X: np.array, weights: np.array, dist_array: np.array, clusters_in_idxs: [[int]]):\n",
    "        centroid_idxs = self._update_centroids(X, clusters_in_idxs)\n",
    "        clusters_in_idxs = self._assign_points_to_clusters(weights, dist_array, centroid_idxs)\n",
    "        maximal_mean_dist_in_clusters = self._get_maximal_mean_dist_in_clusters(dist_array, clusters_in_idxs)        \n",
    "        return clusters_in_idxs, maximal_mean_dist_in_clusters\n",
    "\n",
    "    def _update_centroids(self, X: np.array, clusters_in_idxs: [[int]]) -> [int]:\n",
    "        updated_centroid_idxs = [self._update_centroid_for_one_cluster(X, one_cluster_in_idxs)\n",
    "                                 for one_cluster_in_idxs in clusters_in_idxs]\n",
    "        return updated_centroid_idxs\n",
    "\n",
    "    def _update_centroid_for_one_cluster(self, X: np.array, cluster_in_idxs: [int]) -> int:\n",
    "        center = np.mean(X[cluster_in_idxs], axis=0)\n",
    "        closest_point_idx_in_cluster = np.argmin(np.linalg.norm(X[cluster_in_idxs] - center, axis=1))\n",
    "        closest_point_idx = cluster_in_idxs[closest_point_idx_in_cluster]\n",
    "        return closest_point_idx\n",
    "    \n",
    "class BoundedClustering:\n",
    "    def __init__(self, max_cluster_size: int, n_iter: int = 10, n_init: int = 10, n_k_to_try: int = 10, \n",
    "                 plot_every_iteration: bool = False, plot_edge_values: bool = False):\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "        self.n_iter = n_iter\n",
    "        self.n_init = n_init\n",
    "        self.n_k_to_try = n_k_to_try\n",
    "        self.plot_every_iteration = plot_every_iteration\n",
    "        self.plot_edge_values = plot_edge_values\n",
    "\n",
    "    def fit(self, X: np.array, weights: np.array) -> [float, [[int]]]:\n",
    "        min_n_clusters = int(np.ceil(sum(weights) / self.max_cluster_size))\n",
    "        max_n_clusters = min([min_n_clusters + self.n_k_to_try, len(weights)])\n",
    "        all_k = list(range(min_n_clusters, max_n_clusters + 1))\n",
    "\n",
    "        dist_array = ClustersUtils.compute_dist_array(X)\n",
    "        costs, clusters = zip(*[self._fit_k(k, X, weights, dist_array) for k in all_k])\n",
    "\n",
    "        if np.isnan(np.array(costs)).all():\n",
    "            raise NotEnoughClustersException(f\"Could not produce any clustering with range {all_k}.\")\n",
    "\n",
    "        valid_k_indices, all_points = zip(*[[i, np.array([k, cost])]\n",
    "                                            for i, (k, cost) in enumerate(zip(all_k, costs))\n",
    "                                            if not np.isnan(cost)])\n",
    "        if len(valid_k_indices) <= 2:\n",
    "            i = valid_k_indices[0]\n",
    "            return all_k[i], clusters[i]\n",
    "\n",
    "        p_left, p_right = all_points[0], all_points[-1]\n",
    "        all_distances_from_line = [self._dist_from_line(p_left, p_right, p) for p in all_points]\n",
    "        \n",
    "        best_k_idx_in_all_points = int(np.argmax(all_distances_from_line))\n",
    "        self._plot_elbow(all_points, best_k_idx_in_all_points)\n",
    "        \n",
    "        if self.plot_edge_values:\n",
    "            ClustersUtils.scatter_plot(X, clusters[valid_k_indices[0]])\n",
    "            ClustersUtils.scatter_plot(X, clusters[valid_k_indices[-1]])\n",
    "        \n",
    "        best_k_idx_in_all_ks = valid_k_indices[best_k_idx_in_all_points]\n",
    "        best_n_clusters, best_clusters = all_k[best_k_idx_in_all_ks], clusters[best_k_idx_in_all_ks]\n",
    "        return best_n_clusters, best_clusters\n",
    "    \n",
    "    def _fit_k(self, k: int, X: np.array, weights: np.array, dist_array: np.array = None) -> [float, [[int]]]:\n",
    "        cluster_maker = BoundedKMeansClustering(k, self.max_cluster_size, self.n_iter, self.n_init)\n",
    "        best_cost, best_clusters = cluster_maker.fit(X, weights, dist_array)\n",
    "        return best_cost, best_clusters\n",
    "    \n",
    "    def _dist_from_line(self, p_left: np.array, p_right: np.array, p: np.array) -> float:\n",
    "        # minus sign because we expect the point to be below the line\n",
    "        d = -np.cross(p_right - p_left, p - p_left) / np.linalg.norm(p_right - p_left)\n",
    "        return d\n",
    "    \n",
    "    def _plot_elbow(self, all_points, best_k_idx_in_all_points) -> None:\n",
    "        points_list = np.array(all_points).T.tolist()\n",
    "        plt.plot(points_list[0], points_list[1]);\n",
    "        plt.plot([points_list[0][0], points_list[0][-1]], [points_list[1][0], points_list[1][-1]]);\n",
    "        plt.scatter(all_points[best_k_idx_in_all_points][0], all_points[best_k_idx_in_all_points][1]);\n",
    "        plt.xlabel('Number of clusters');\n",
    "        plt.ylabel('Cost');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WRITTEN BY THE AUTHORS\n",
    "\n",
    "def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n",
    "                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n",
    "                     bbox=[0, 0, 1, 1], header_columns=0,\n",
    "                     ax=None, **kwargs):\n",
    "    \n",
    "    # saves pandas dataframe as figure\n",
    "    \n",
    "    if ax is None:\n",
    "        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n",
    "        fig, ax = plt.subplots(figsize=size)\n",
    "        ax.axis('off')\n",
    "    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n",
    "    mpl_table.auto_set_font_size(False)\n",
    "    mpl_table.set_fontsize(font_size)\n",
    "\n",
    "    for k, cell in mpl_table._cells.items():\n",
    "        cell.set_edgecolor(edge_color)\n",
    "        if k[0] == 0 or k[1] < header_columns:\n",
    "            cell.set_text_props(weight='bold', color='w')\n",
    "            cell.set_facecolor(header_color)\n",
    "        else:\n",
    "            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n",
    "    return ax.get_figure(), ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_returns(x, y):\n",
    "    # save returns for each cluster in dictionary\n",
    "    # takes as input a dataframe containing the assets and corresponding cluster index (x) and a matrix of returns (y)\n",
    "    # returns a dictionary containing the returns for each cluster\n",
    "    x.columns = [\"Asset\",\"Cluster\"]\n",
    "    (unique, counts) = np.unique(x[\"Cluster\"], return_counts=True)\n",
    "    \n",
    "    ret_dict = {}\n",
    "    for i in unique: \n",
    "        lgc = x[\"Cluster\"] == i\n",
    "        ret_dict[i] = y.loc[:,lgc.values]\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_weights(x):\n",
    "    # computes equally weighted portfolio returns for each cluster\n",
    "    # takes a dictionary containing the cluster returns as input (output of cluster_returns)\n",
    "    # returns a dataframe containing the equally weighted portfolio returns\n",
    "    ret_ew = pd.DataFrame()\n",
    "\n",
    "    for i in x.keys(): \n",
    "        ret_ew = ret_ew.append(x[i].mean(axis = 1), ignore_index = True)\n",
    "\n",
    "    ret_ew = ret_ew.T\n",
    "    \n",
    "    return(ret_ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_portfolio(x, clust_type):\n",
    "    # computes optimal portfolio weights on 252 days rolling windows (daily portfolio rebalancing)\n",
    "    # takes as input a dataframe of cluster returns (x = output of equal_weights) \n",
    "    # and a string with the type of clustering (clust_type = \"partitional\" or \"hierarchical\")\n",
    "    # returns dataframe of optimal portfolio weights for each cluster\n",
    "    window = 252\n",
    "    \n",
    "    pesi_dict = {}\n",
    "    dates_dict = {}\n",
    "\n",
    "    for i in range( len(x) - window - 1 ):\n",
    "        \n",
    "        rets_rolling = x.iloc[i:(i+window),:] \n",
    "\n",
    "        if clust_type == \"hierarchical\":\n",
    "            \n",
    "            hrp = hierarchical_portfolio.HRPOpt(rets_rolling)\n",
    "            raw_weights = hrp.optimize()\n",
    "            weights_fin = hrp.clean_weights()\n",
    "            \n",
    "        else:\n",
    "            # expected returns and sample covariance \n",
    "            cluster_price = expected_returns.prices_from_returns(rets_rolling)\n",
    "            mu = expected_returns.mean_historical_return(cluster_price)\n",
    "            S = risk_models.sample_cov(cluster_price)\n",
    "            \n",
    "            try:\n",
    "                # maximum sharpe ratio (tangency) portfolio weights \n",
    "                ef = EfficientFrontier(mu, S)\n",
    "                raw_weights = ef.max_sharpe()\n",
    "                weights_fin = ef.clean_weights()\n",
    "\n",
    "            except:\n",
    "                ef = EfficientFrontier(mu, S, solver = cvxpy.installed_solvers()[0])\n",
    "                raw_weights = ef.max_sharpe()\n",
    "                weights_fin = ef.clean_weights()\n",
    "\n",
    "        pesi_dict[i] = weights_fin.values()\n",
    "        dates_dict[i] = rets_rolling.index[-1] \n",
    "\n",
    "    pesi_df = pd.DataFrame(pesi_dict.values())\n",
    "    pesi_df.index = pd.Index(dates_dict.values())\n",
    "    pesi_df.columns = x.columns\n",
    "\n",
    "    return(pesi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_portfolio(x):\n",
    "    # performs static portfolio optimization\n",
    "    # takes dataframe of cluster returns as input (output of equal_weights)\n",
    "    # returns dictionary of optimal portfolio weights for each cluster\n",
    "\n",
    "    cluster_price = expected_returns.prices_from_returns(x)\n",
    "    mu = expected_returns.mean_historical_return(cluster_price)\n",
    "    S = risk_models.sample_cov(cluster_price)\n",
    "\n",
    "    ef = EfficientFrontier(mu, S)\n",
    "    raw_weights = ef.max_sharpe()\n",
    "    weights = ef.clean_weights()\n",
    "    \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_returns(pf_returns, d): \n",
    "    # computes cumulative returns\n",
    "    # takes as input a vector of portfolio returns (pf_returns) and an index of test set dates (d)\n",
    "    # returns dataframe of cumulative returns on test set\n",
    "    \n",
    "    # select correct dates\n",
    "    lgc = pf_returns.index.isin(d)\n",
    "    ret_fin = pf_returns.iloc[lgc]\n",
    "\n",
    "    # cumulative returns\n",
    "    cumulative = pd.DataFrame( 100*( (ret_fin + 1).cumprod() - 1 ) )\n",
    "    \n",
    "    return(cumulative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_sharpe(rets, rf_rate, d):\n",
    "    # computes portfolio excess returns wrt the risk free rate and the annualized sharpe ratio\n",
    "    # takes as input a serie of daily returns (rets), a series of risk free rates (rf_rate)\n",
    "    # and an index of test set dates (d)   \n",
    "    \n",
    "    lgc = rf_rate.index.isin(d)\n",
    "    rf_subs = rf_rate.iloc[lgc, :]\n",
    "    \n",
    "    diff_rf = rets - rf_subs[\"DTB3A\"]\n",
    "    \n",
    "    ann_sr = sqrt(252)*(diff_rf.mean())/(rets.std())\n",
    "    return(diff_rf, ann_sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_moments(rets):\n",
    "    # computes annualized mean and standard deviation of returns\n",
    "    # takes as input a series of daily returns\n",
    "    \n",
    "    ann_std = rets.std()*sqrt(252)\n",
    "    ann_er = rets.mean()*252\n",
    "    \n",
    "    return(ann_er, ann_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "Historical returns from about 200 stocks from Nasdaq, chosen randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2010,1,1)\n",
    "end = datetime(2020,12,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download symbols of all nasdaq components\n",
    "url=\"https://pkgstore.datahub.io/core/nasdaq-listings/nasdaq-listed_csv/data/7665719fb51081ba0bd834fde71ce822/nasdaq-listed_csv.csv\"\n",
    "s = requests.get(url).content\n",
    "companies = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "symbols = companies['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 500 assets\n",
    "random.seed(123)\n",
    "Symbols = random.sample(symbols, 500)\n",
    "\n",
    "# download stock prices\n",
    "stock_final = pd.DataFrame()\n",
    "\n",
    "for i in Symbols:  \n",
    "    try:\n",
    "        stock = []\n",
    "        stock = yf.download(i,start=start, end=end, progress=False)\n",
    "        \n",
    "        if len(stock) == 0:\n",
    "            None\n",
    "        else:\n",
    "            stock['Name']= i\n",
    "            stock_final = stock_final.append(stock,sort=False)\n",
    "    except Exception:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of closing prices \n",
    "close = stock_final[[\"Close\", \"Name\"]]\n",
    "close_wide = close.pivot_table(index=\"Date\", columns='Name', values='Close')\n",
    "stock_price = close_wide.dropna(axis = 1)\n",
    "\n",
    "# number of stocks \n",
    "n_stocks = len(stock_price.columns)\n",
    "\n",
    "# stock returns\n",
    "returns = stock_price.pct_change().iloc[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns.to_pickle(\"./returns.pkl\")x\n",
    "\n",
    "# import returns\n",
    "returns_imp = pd.read_pickle('returns.pkl')\n",
    "\n",
    "# import 3 month tbill\n",
    "tbills = pd.read_pickle('DTB3.pkl')\n",
    "\n",
    "# annualized tbill \n",
    "annualized = []\n",
    "\n",
    "for i in tbills['DTB3']:\n",
    "    try:\n",
    "       annualized.append((1 + float(i))**(1/252) - 1)\n",
    "    except ValueError:\n",
    "       annualized.append(0.0)\n",
    "\n",
    "tbills['DTB3A'] = annualized\n",
    "\n",
    "tbills.index = pd.to_datetime(tbills.index)\n",
    "tbills.index.names = ['Date']\n",
    "merged = pd.merge(tbills, returns_imp, on='Date')\n",
    "tbill_ann = pd.DataFrame( merged[\"DTB3A\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual mean returns and variances \n",
    "mean_ret, var_ret = ann_moments(returns_imp)\n",
    "\n",
    "rets_df = pd.concat([mean_ret, var_ret], axis = 1)\n",
    "rets_df.columns = [\"Returns\",\"Volatility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect outliers\n",
    "X = rets_df.values \n",
    "pl.scatter(X[:,0],X[:,1])\n",
    "pl.xlabel(\"mean\")\n",
    "pl.ylabel(\"volatility\")\n",
    "pl.savefig('returns_outliers.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "xx = pd.DataFrame(mean_ret)\n",
    "xx_sorted = xx.sort_values(by = 0)[-2:].index\n",
    "outliers = list(xx_sorted)\n",
    "\n",
    "returns = returns_imp.drop(outliers, 1)\n",
    "rets_df2 = rets_df.drop(outliers)\n",
    "\n",
    "X = rets_df2.values \n",
    "pl.scatter(X[:,0],X[:,1])\n",
    "pl.xlabel(\"mean\")\n",
    "pl.ylabel(\"volatility\")\n",
    "pl.savefig('returns_outliers2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test set \n",
    "cutoff = \"2019-12-31\"\n",
    "\n",
    "ret_train = returns[returns.index <= cutoff]\n",
    "\n",
    "ret_test = returns[returns.index > (datetime.strptime(cutoff, '%Y-%m-%d') -  relativedelta(years=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual mean returns and variances \n",
    "mean_ret, var_ret = ann_moments(ret_train)\n",
    "\n",
    "rets_df = pd.concat([mean_ret, var_ret], axis = 1)\n",
    "rets_df.columns = [\"Returns\",\"Volatility\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select optimal number of clusters by minimizing SSE\n",
    "X =  rets_df.values \n",
    "sse = []\n",
    "\n",
    "random.seed(123)\n",
    "for k in range(2,15):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    sse.append(kmeans.inertia_) #SSE for each cluster\n",
    "    \n",
    "pl.plot(range(2,15), sse)\n",
    "pl.title(\"Elbow Curve\")\n",
    "pl.xlabel('nr clusters')\n",
    "pl.ylabel('SSE')\n",
    "pl.axvline(x=5, c = \"k\", linestyle='dashed')\n",
    "pl.savefig('elbow_rule_kmeans.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit k-means with 5 clusters\n",
    "X = rets_df.values \n",
    "n_clusters = 5\n",
    "\n",
    "kmeans = KMeans(n_clusters = n_clusters).fit(X)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "scatter = ax.scatter(X[:,0],X[:,1], c = kmeans.labels_, cmap = \"rainbow\")\n",
    "ax.legend(*scatter.legend_elements(), loc='upper left', bbox_to_anchor=(1, 1))\n",
    "pl.savefig('kmeans.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of elements in each cluster\n",
    "cluster_idx = np.array(kmeans.labels_)\n",
    "(unique, counts) = np.unique(cluster_idx, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "\n",
    "# get cluster number for each asset\n",
    "asset = pd.DataFrame(rets_df.index)\n",
    "cluster_list = pd.concat([asset, pd.DataFrame(cluster_idx)],axis = 1)\n",
    "cluster_list.columns = [\"Asset\",\"Cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio optimization K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save return series for each cluster in dictionary\n",
    "ret_dict = cluster_returns(x = cluster_list, y = ret_test)\n",
    "\n",
    "# compute equally weighted portfolio returns for each cluster\n",
    "ret_ew = equal_weights(ret_dict)\n",
    "\n",
    "# optimal portfolio weights on 252 days rolling windows (daily portfolio rebalancing)\n",
    "pesi_rol = rolling_portfolio(ret_ew, clust_type = \"partitional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure return dates match weight dates \n",
    "lgc = ret_ew.index.isin(pesi_rol.index)\n",
    "ret_subs = ret_ew.iloc[lgc, :]\n",
    "\n",
    "# select dates for portfolio backtest\n",
    "dates_backtest = ret_subs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "kmeans_rets = (ret_subs*pesi_rol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "kmeans_rol = cumulative_returns(pf_returns = kmeans_rets, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static portfolio optimization K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio optimization\n",
    "pesi_static = static_portfolio(ret_ew)\n",
    "\n",
    "# portfolio returns\n",
    "kmeans_rets_static = (ret_ew*list(pesi_static.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "kmeans_static = cumulative_returns(pf_returns = kmeans_rets_static, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounded K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rets_df.values\n",
    "\n",
    "n_clusters = 5\n",
    "n_iter = 10\n",
    "weights = np.ones(len(X)) # weight of individual observations set to one \n",
    "max_cluster_size = (sum(weights)// n_clusters) + n_clusters\n",
    "\n",
    "random.seed(123)\n",
    "cluster_maker = BoundedClustering(max_cluster_size, n_iter) \n",
    "best_k, best_clusters = cluster_maker.fit(X, weights)\n",
    "print(f\"Best partition was found for {best_k} clusters\")\n",
    "print(f\"Total weight = {sum(weights)}\")\n",
    "print(f\"Max_cluster_size = {max_cluster_size}\")\n",
    "print(f\"Clusters weights: {dict((i, sum(weights[c])) for i, c in enumerate(best_clusters))}\")\n",
    "ClustersUtils.scatter_plot(X, best_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster number for each asset\n",
    "\n",
    "dict_bkm = {}\n",
    "for clu, assets in enumerate (best_clusters):\n",
    "    dict_bkm[clu] = assets\n",
    "\n",
    "cluster_array = np.zeros(len(X))\n",
    "\n",
    "for k, it in dict_bkm.items():\n",
    "    for i in it:\n",
    "        cluster_array[i] = k\n",
    "\n",
    "asset = pd.DataFrame(rets_df.index)\n",
    "cluster_bkm = pd.concat([asset, pd.DataFrame(cluster_array)], axis = 1)\n",
    "cluster_bkm.columns = ['assets', 'clusters bkm']\n",
    "cluster_bkm['clusters bkm'] = cluster_bkm['clusters bkm'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio Bounded K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save return series for each cluster in dictionary\n",
    "ret_dict = cluster_returns(x = cluster_bkm, y = ret_test)\n",
    "\n",
    "# compute equally weighted portfolio returns for each cluster\n",
    "ret_ew = equal_weights(ret_dict)\n",
    "\n",
    "# optimal rolling portfolio weights\n",
    "pesi_rol_bkm = rolling_portfolio(ret_ew, clust_type = \"partitional\")\n",
    "\n",
    "# make sure return dates match weight dates \n",
    "lgc = ret_ew.index.isin(pesi_rol_bkm.index)\n",
    "ret_subs = ret_ew.iloc[lgc, :]\n",
    "\n",
    "# select dates for portfolio backtest\n",
    "dates_backtest = ret_subs.index\n",
    "\n",
    "# portfolio returns\n",
    "bkm_rets = (ret_subs*pesi_rol_bkm).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "bkm_rol = cumulative_returns(pf_returns = bkm_rets, d = dates_backtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Portfolio Optimization Bounded K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio optimization\n",
    "pesi_bkm_static = static_portfolio(ret_ew)\n",
    "\n",
    "# portfolio returns\n",
    "bkm_rets_static = (ret_ew*list(pesi_bkm_static.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "bkm_static = cumulative_returns(pf_returns = bkm_rets_static, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical risk parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrp clusters (from pyhrp package)\n",
    "cov, cor = ret_train.cov(), ret_train.corr()\n",
    "links = linkage(dist(cor.values), method='ward')\n",
    "node = tree(links)\n",
    "\n",
    "#rootcluster = _hrp(node, cov)\n",
    "#weights_hrp = rootcluster.weights\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax = dendrogram(links, orientation=\"top\")\n",
    "pl.savefig('dendogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static portfolio optimization HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static portfolio weights (from pypfopt package)\n",
    "hrp = hierarchical_portfolio.HRPOpt(ret_train)\n",
    "raw_weights = hrp.optimize()\n",
    "weights_hrp = hrp.clean_weights()\n",
    "# cluster = hrp.clusters\n",
    "\n",
    "# match dates\n",
    "lgc = ret_test.index.isin(dates_backtest)\n",
    "ret_subs = ret_test.iloc[lgc, :]\n",
    "\n",
    "# portfolio returns\n",
    "hrp_rets_static = (ret_subs*list(weights_hrp.values())).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "hrp_static = cumulative_returns(pf_returns = hrp_rets_static, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling portfolio optimization HRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hrp_pesirol = rolling_portfolio(ret_test, clust_type = \"hierarchical\")\n",
    "\n",
    "#hrp_pesirol.to_pickle(\"./pesi_rolling_hrp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rolling hrp weights\n",
    "hrp_pesirol = pd.read_pickle(\"./pesi_rolling_hrp.pkl\")\n",
    "\n",
    "hrp_pesirol.columns = ret_test.columns\n",
    "\n",
    "# portfolio returns\n",
    "hrp_rets_rol = (ret_subs*hrp_pesirol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "hrp_rol = cumulative_returns(pf_returns = hrp_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling tangency portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tangency_rol = rolling_portfolio(ret_test, clust_type = \"none\")\n",
    "\n",
    "#tangency_rol.to_pickle(\"./pesi_rolling_tangency.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import rolling tangency weights\n",
    "tan_pesirol = pd.read_pickle(\"./pesi_rolling_tangency.pkl\")\n",
    "\n",
    "# portfolio returns\n",
    "tan_rets_rol = (ret_subs*tan_pesirol).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "tan_rol = cumulative_returns(pf_returns = tan_rets_rol, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static tangency portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio weights\n",
    "cluster_price = expected_returns.prices_from_returns(ret_test)\n",
    "mu = expected_returns.mean_historical_return(cluster_price)\n",
    "S = risk_models.sample_cov(cluster_price)\n",
    "ef = EfficientFrontier(mu, S)\n",
    "raw_weights = ef.max_sharpe()\n",
    "tan_pesi_static = ef.clean_weights()\n",
    "\n",
    "# portfolio returns\n",
    "tan_rets_st = (ret_subs*tan_pesi_static).sum(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "tan_static = cumulative_returns(pf_returns = tan_rets_st, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equally weighted portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio returns\n",
    "ew_rets = ret_subs.mean(axis = 1)\n",
    "\n",
    "# cumulative returns\n",
    "ew_rol = cumulative_returns(pf_returns = ew_rets, d = dates_backtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rets_df.values\n",
    "\n",
    "# silhouette coefficient \n",
    "km_ss = silhouette_score(X, kmeans.labels_, metric = 'euclidean')\n",
    "\n",
    "bkm_ss = silhouette_score(X, cluster_array.astype(int), metric = 'euclidean')\n",
    "\n",
    "# calinski_harabasz_score\n",
    "km_chs = calinski_harabasz_score(X, kmeans.labels_)\n",
    "\n",
    "bkm_chs = calinski_harabasz_score(X, cluster_array.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute portfolio excess returns wrt the risk free rate and the annualized sharpe ratios\n",
    "\n",
    "names = [\"K-Means\", \"Bounded K-Means\", \"HRP\", \"Tangency\", \"Equal Weights\"]\n",
    "rets_list = [kmeans_rets, bkm_rets, hrp_rets_rol, tan_rets_rol, ew_rets]\n",
    "\n",
    "for i in range (len(names)): \n",
    "    \n",
    "    if i == 0:\n",
    "        output = ann_sharpe(rets = rets_list[i], rf_rate = tbill_ann, d = dates_backtest)\n",
    "        exc_ret = output[0]\n",
    "        sharpe = np.array(output[1]) \n",
    "    else: \n",
    "        output = ann_sharpe(rets = rets_list[i], rf_rate = tbill_ann, d = dates_backtest)\n",
    "        exc_ret = pd.concat( [exc_ret, output[0] ], axis = 1)\n",
    "        sharpe = np.append( sharpe, output[1] ) \n",
    "        \n",
    "exc_ret.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute annualized portfolio expected returns and volatility\n",
    "\n",
    "rets_list = [kmeans_rets, bkm_rets, hrp_rets_rol, tan_rets_rol, ew_rets]\n",
    "\n",
    "for i in range (len(names)): \n",
    "    \n",
    "    if i == 0:\n",
    "        moments = pd.DataFrame(ann_moments(rets_list[i]))\n",
    "    else: \n",
    "        moments = pd.concat( [moments, pd.DataFrame(ann_moments(rets_list[i]))], axis = 1)\n",
    "        \n",
    "moments.columns = names\n",
    "moments.index = [\"returns\", \"volatility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute difference between static and rolling portfolio returns\n",
    "rol_list = [kmeans_rol, bkm_rol, hrp_rol, tan_rol]\n",
    "static_list = [kmeans_static, bkm_static, hrp_static, tan_static]\n",
    "\n",
    "for i in range(len(rol_list)):\n",
    "    \n",
    "    if i == 0:\n",
    "        diff_rol = rol_list[i] - static_list[i]\n",
    "    else:\n",
    "        diff_rol = pd.concat( [diff_rol, rol_list[i] - static_list[i]], axis = 1)\n",
    "\n",
    "diff_rol.columns = names[:-1]\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "plt.plot(diff_rol)\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Differenziale fra il rendimento di un portafoglio statico e rolling\")\n",
    "plt.legend(loc='upper left', labels = names[:-1])\n",
    "pl.savefig('excess_rolling_static.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cumulative excess return with respect to risk free rate\n",
    "for i in range(len(names)):\n",
    "    \n",
    "    if i == 0:\n",
    "        cumret_df = cumulative_returns(pf_returns = exc_ret.iloc[:, i], d = dates_backtest)\n",
    "    else:\n",
    "        cumret_df = pd.concat( [cumret_df, \n",
    "                                cumulative_returns(pf_returns = exc_ret.iloc[:, i], d = dates_backtest)], \n",
    "                              axis = 1)\n",
    "\n",
    "cumret_df.columns = names\n",
    "\n",
    "# figure\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_axes([0.1,0.1,0.8,0.8])\n",
    "plt.plot(cumret_df)\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax1.set_title(\"Extrarendimento rispetto al tasso privo di rischio\")\n",
    "plt.legend(loc='upper left', labels = names)\n",
    "pl.savefig('excess_return.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table kmeans static weights\n",
    "my_dict = dict(pesi_static)\n",
    "my_dict.update((x, y*100) for x, y in my_dict.items())\n",
    "\n",
    "kmeans_tab = pd.DataFrame( )\n",
    "kmeans_tab[\"cluster\"] = dict(frequencies).keys()\n",
    "kmeans_tab[\"nr elements\"] = dict(frequencies).values()\n",
    "kmeans_tab[\"static weights\"] = my_dict.values()\n",
    "\n",
    "fig,ax = render_mpl_table(kmeans_tab.round(), header_columns=0, col_width=2.0)\n",
    "fig.savefig(\"table_kmeans.png\")\n",
    "\n",
    "# table kmeans rolling weights\n",
    "dates = pesi_rol.index.date\n",
    "\n",
    "tab_km = pd.DataFrame()\n",
    "tab_km[\"date\"] = [str(i) for i in dates]\n",
    "tab_km.index = pesi_rol.index\n",
    "\n",
    "tab_km_rol = pd.concat([tab_km, (pesi_rol*100)], axis = 1)\n",
    "\n",
    "fig,ax = render_mpl_table(tab_km_rol.iloc[:10].round(), header_columns=0, col_width=2.0)\n",
    "fig.savefig(\"table_km_rolling.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table bounded-kmeans static weights\n",
    "my_dict = dict(pesi_bkm_static)\n",
    "my_dict.update((x, y*100) for x, y in my_dict.items())\n",
    "\n",
    "cluster_weights = dict((i, sum(weights[c])) for i, c in enumerate(best_clusters))\n",
    "\n",
    "b_kmeans_tab = pd.DataFrame( )\n",
    "b_kmeans_tab[\"cluster\"] = cluster_weights.keys()\n",
    "b_kmeans_tab[\"nr elements\"] = cluster_weights.values()\n",
    "b_kmeans_tab[\"static weights\"] = my_dict.values()\n",
    "\n",
    "fig,ax = render_mpl_table(b_kmeans_tab.round(), header_columns=0, col_width=2.0)\n",
    "fig.savefig(\"table_b_kmeans.png\")\n",
    "\n",
    "# table b-kmeans rolling weights\n",
    "dates = pesi_rol_bkm.index.date\n",
    "\n",
    "tab_bkm = pd.DataFrame()\n",
    "tab_bkm[\"date\"] = [str(i) for i in dates]\n",
    "tab_bkm.index = pesi_rol_bkm.index\n",
    "\n",
    "tab_bkm_rol = pd.concat([tab_bkm, (pesi_rol_bkm*100)], axis = 1)\n",
    "\n",
    "fig,ax = render_mpl_table(tab_bkm_rol.iloc[:10].round(), header_columns=0, col_width=2.0)\n",
    "fig.savefig(\"table_bkm_rolling.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table hrp rolling weights\n",
    "dates = hrp_pesirol.index.date\n",
    "\n",
    "tab_hrp = pd.DataFrame()\n",
    "tab_hrp[\"date\"] = [str(i) for i in dates]\n",
    "tab_hrp.index = hrp_pesirol.index\n",
    "\n",
    "tab_hrp_rol = pd.concat([tab_hrp, (hrp_pesirol*100)], axis = 1)\n",
    "\n",
    "fig,ax = render_mpl_table(tab_hrp_rol.iloc[:10,:10].round(2), header_columns=0, col_width=2.0)\n",
    "fig.savefig(\"table_hrp_rolling.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table tangency portfolio rolling weigths\n",
    "dates = tan_pesirol.index.date\n",
    "\n",
    "tab_tan = pd.DataFrame()\n",
    "tab_tan[\"date\"] = [str(i) for i in dates]\n",
    "tab_tan.index = tan_pesirol.index\n",
    "\n",
    "tab_tan_rol = pd.concat([tab_tan, (tan_pesirol*100)], axis = 1)\n",
    "\n",
    "fig,ax = render_mpl_table(tab_tan_rol.iloc[:10,:10].round(2), header_columns=0, col_width=2.0)\n",
    "fig.savefig(\"table_tan_rolling.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of cumulative returns, sharpe ratios, expected returns and volatility\n",
    "names_rows = ['Cumulative Returns', 'Ann. Sharpe Ratios', \"Ann. Expected Returns\", \"Ann. Volatility\"]\n",
    "\n",
    "table_cumret =  pd.DataFrame( cumret_df.iloc[-1,:].round()).T\n",
    "\n",
    "final_table = pd.concat([table_cumret, sharpe, moments], axis = 0)\n",
    "\n",
    "col_name = pd.DataFrame(names_rows)\n",
    "tab_print = pd.merge(col_name, final_table, on = col_name.index)\n",
    "tab_print = tab_print.drop(['key_0'], axis = 1)\n",
    "tab_print.columns = [\" \"] + names\n",
    "fig,ax = render_mpl_table(tab_print.round(2), header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"table_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster evaluation table\n",
    "\n",
    "km_ev = [km_ss, km_chs]\n",
    "bkm_ev = [bkm_ss, bkm_chs]\n",
    "ev_type = ['Silhouette coefficient', 'Calinski-Harabasz Index']\n",
    "ev_df = pd.DataFrame([ev_type, [round(num, 2) for num in km_ev], [round(num, 2) for num in bkm_ev]]).T\n",
    "ev_df.columns = [\" \", \"K-Means\", \"Bounded K-Means\"]\n",
    "\n",
    "fig,ax = render_mpl_table(ev_df, header_columns=0, col_width=4.0)\n",
    "fig.savefig(\"table_cluster_ev.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print full size table\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    print (pesi_rol)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
